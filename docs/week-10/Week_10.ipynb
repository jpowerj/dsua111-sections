{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 10: Evaluating and Interpreting Linear Regression, Machine Learning\n",
    "\n",
    "## DSUA111: Data Science for Everyone, NYU, Fall 2020\n",
    "\n",
    "### TA Jeff, `jpj251@nyu.edu`\n",
    "\n",
    "* This slideshow: https://jjacobs.me/dsua111-sections/week-10\n",
    "* All materials: https://github.com/jpowerj/dsua111-sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "\n",
    "I. Evaluating and Interpreting Linear Regression\n",
    "\n",
    "0. Coefficient Interpretation\n",
    "1. p-value Interpretation\n",
    "2. Evaluating Results\n",
    "\n",
    "II. Machine Learning (aka Fancy Regressions)\n",
    "\n",
    "0. Statistics vs. Machine Learning?\n",
    "1. ML Approach to Regression\n",
    "2. Evaluating Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part I: Evaluating and Interpreting Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## *Substantive* vs. *Significant* Effects\n",
    "\n",
    "* ***Substantive***: Is it interesting for society?\n",
    "* ***Significant***: How much do we believe it?\n",
    "* Example: A government carries out two studies:\n",
    "    * One finds that a means-tested welfare program decreases poverty by 0.00001%, significant at the 1% level of confidence.\n",
    "    * The other finds that a universal basic income program decreases poverty by 10%, significant at the 10% level of confidence (but not 5% or 1%)\n",
    "* Which is more substantive? Which is more significant?\n",
    "* [October 16, 2020: \"JFI, Mayor Aja Brown, and Fund for Guaranteed Income announce largest city-based guaranteed income initiative\"](https://www.jainfamilyinstitute.org/news/jfi-mayor-aja-brown-and-fund-for-guaranteed-income-announce-largest-city-based-guaranteed-income-initiative/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Justify Your Measures!\n",
    "\n",
    "<center>\n",
    "<img src=\"justify_your_measures.png\" style=\"width:80%;\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## p-values\n",
    "\n",
    "[In theory,] **\"the probability of getting the coefficient we got if the null hypothesis is correct\"**\n",
    "* In regression analyses, null hypothesis is almost always \"coefficient of interest is 0\"\n",
    "* Hence, in regression the p-value is \"the probability we'd get the *observed coefficient* (the coefficient in the regression results table) if the *true coefficient* is 0\"\n",
    "* $\\implies$ the smaller the p-value is, the more we \"believe\" that the effect we found is real\n",
    "\n",
    "In practice, too many \"moving parts\" in experiments (and ESPECIALLY in observational studies) for this to be that meaningful\n",
    "* Bayesian statistical approach used more and more since ~the 80s, since computers allow us to compute more subtle measures/incorporate more details and prior knowledge than the (frequentist) p-value\n",
    "* e.g., if 1 million experiments up til now found the gravitational constant to be $6.67408 \\times 10^{-11}$, but we do an experiment which results in an estimate of $3$, Bayesian approach allows us to view this result more skeptically given *prior* information/data\n",
    "* [Gelman, Andrew. \"P Values and Statistical Practice\". *Epidemiology* 24(1): 69-72. 2013](http://www.stat.columbia.edu/~gelman/research/published/pvalues3.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## OLS Diagnostics\n",
    "\n",
    "In this class we focus on:\n",
    "\n",
    "* R-squared\n",
    "* Adj. R-squared\n",
    "* Prob(F-statistic)\n",
    "* Confidence interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## R-Squared\n",
    "\n",
    "* In theory, **\"The proportion of variance in the dependent variable 'explained by' the independent variable(s)\"**\n",
    "* In practice, \"explained by\" is a loaded term, and honestly I would just think of it like a regression-specific correlation measure\n",
    "* \"Adjusted\" R-Squared just applies a penalty for each additional variable you include in your model (since you can always make non-adjusted R-squared increase by adding more and more variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## F-Statistic\n",
    "\n",
    "* The p-value is specific to *one* coefficient, on one independent variable (\"how significant is this coefficient/this independent variable's effect on the dependent variable\")\n",
    "* The F-statistic can be thought of like a p-value for your *whole* model, for *all* coefficients/independent variables. Similarly to the p-value, the F-statistic value is the probability of getting your model if *all* coefficients were 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Confidence Intervals\n",
    "\n",
    "* \"Confidence intervals tell us: **If we conduct our study many, many times, 95% of the time our confidence interval will capture the true coefficient**\"\n",
    "* \"Important: Confidence intervals **do not** say that we are **'95% confident our coefficient is within this range'**\"\n",
    "* (Jeff's 2 cents: more trouble than they're worth... see Gelman article above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multiple Regression\n",
    "\n",
    "* You can have more than one independent variable!\n",
    "* $Y_i = \\beta_0 + \\beta_1x_{1,i} + \\beta_2x_{2,i} + \\ldots + \\beta_Nx_{N,i}$\n",
    "    * $x_{1,i}$: 1st independent variable (its value for person $i$)\n",
    "    * $x_{2,i}$: 2nd independent variable (its value for person $i$)\n",
    "    * $\\cdots$\n",
    "    * $x_{N,i}$: $N$th independent variable (its value for person $i$)\n",
    "* Now, the coefficient interpretation changes a bit:\n",
    "    * An increase of 1 unit in $x_{1}$ is associated with an increase of $\\beta_1$ units in $Y$, **when all other variables are held constant**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part II: Machine Learning (aka Fancy Regressions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Statistics vs. Machine Learning\n",
    "\n",
    "Biggest difference has to do with the respective goals of the two approaches:\n",
    "\n",
    "* **Statistics**: Inference (Why is something happening? What is the relationship between variables of interest? What causes economic growth?)\n",
    "* **Machine Learning**: Prediction (What is going to happen next? Will people who do X also do Y? What will be the level of the stock market one month from today?\n",
    "\n",
    "(from Lecture 18.1, Slide 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Difference in Practice\n",
    "\n",
    "* You've already seen \"statistical\" regression, where you toss all the data into `statsmodels` and observe the result\n",
    "* In Machine Learning, it's a bit more involved:\n",
    "    0. Split the data into training and testing sets (typically 80% of the data for training, 20% for testing)\n",
    "    1. **Train** the model on the training set\n",
    "    2. **Evaluate** the model on the testing set\n",
    "    3. Make **predictions** (but perhaps don't learn as much about the underlying relationships between variables)\n",
    "* [Note: SUPER important that the ML algorithm **never** sees the test data]\n",
    "    \n",
    "(from Lecture 18.1, Slide 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `scikit-learn`\n",
    "\n",
    "0. `import sklearn`\n",
    "1. Load data\n",
    "2. Split into training (80%) and test (20%) sets\n",
    "3. **Train** the model\n",
    "    * (What's going on \"under the hood\" here? Actually not that scary and worth knowing!)\n",
    "4. Ask the model to predict **dependent variable** values given **independent variable values** in the **test set**\n",
    "5. Compute **accuracy** by comparing these **predicted** dependent variable values with the **actual** dependent variable values in the test set (important: the ML model never ever looks at these!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computing Accuracy\n",
    "\n",
    "<center>\n",
    "<img src=\"accuracy.png\" style=\"width:80%;\">\n",
    "</center>\n",
    "\n",
    "(Lecture 18.2, Slide 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Surprisingly Helpful: Predicted vs. Actual Values Table\n",
    "\n",
    "<center>\n",
    "<img src=\"predicted_vs_actual.png\" style=\"width:80%;\">\n",
    "</center>\n",
    "\n",
    "(Lecture 18.3, Slide 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Less-Surprisingly Helpful: Visualizing Accuracy\n",
    "\n",
    "<center>\n",
    "<img src=\"predicted_vs_actual_plot.png\" style=\"width:80%;\">\n",
    "</center>\n",
    "\n",
    "(Lecture 18.3, Slide 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## OVERFITTING: Why you should NOT be maximizing accuracy\n",
    "\n",
    "<center>\n",
    "<img src=\"1024px-Overfitting.svg.png\" style=\"width:50%;\">\n",
    "</center>\n",
    "\n",
    "(from https://en.wikipedia.org/wiki/Overfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to Know When You're Overfitting\n",
    "\n",
    "<center>\n",
    "<img src=\"overfitting.jpeg\" style=\"width:66.6%;\">\n",
    "</center>\n",
    "\n",
    "(from https://medium.com/ai%C2%B3-theory-practice-business/dropout-early-stopping-188a23beb2f9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Week_10.ipynb to slides\n",
      "[NbConvertApp] Writing 291987 bytes to Week_10.slides.html\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert Week_10.ipynb --to slides --SlidesExporter.reveal_scroll=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
